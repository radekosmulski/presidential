{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk like the President, part 2\n",
    "\n",
    "Welcome back!\n",
    "\n",
    "In this part we will continue to make changes to our model. Hopefully we will end up with a model that can take much longer sequences into account and that is also easier to train.\n",
    "\n",
    "Additionally, in order to improve the quality of generated texts we will try our hand at beam search.\n",
    "\n",
    "Without further ado, let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from utils import *\n",
    "\n",
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.column_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 74\n",
      "Mr. Speaker, Mr. Vice President, members of Congress, distinguished guests, and fellow Americans: La\n"
     ]
    }
   ],
   "source": [
    "# You should already have the speeches downloaded. If not, please run the code in part 1\n",
    "\n",
    "# Loading in speeches and doing the usual maintance work\n",
    "speeches = ' '.join(preprocess_speech(file) for file in glob('data/Corpus of Presential Speeches/obama/*'))\n",
    "\n",
    "chars = sorted(list(set(speeches)))\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', vocab_size) # Sanity check 1\n",
    "\n",
    "char2idx = {c: idx for idx, c in enumerate(chars)}\n",
    "idx2char = {idx: c for idx, c in enumerate(chars)}\n",
    "idxs = [char2idx[char] for char in speeches]\n",
    "print(''.join(idx2char[idxs[i]] for i in range(100))) # Sanity check 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-output model\n",
    "\n",
    "So far we have been running the model on some amount of chars and predicting the next one. Then we repeat the process - we run the model on n characters again and predict the next one.\n",
    "\n",
    "This seems to be wasting a lot of opportunities to train, especially with longer sequences. \n",
    "\n",
    "Could we also predic the intermediate chars in the sequence and train on those? See 1st char, predict 2nd char, see 2nd char, predict 3rd char, and so on.\n",
    "\n",
    "Let's start with creating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dat = [idxs[i:i+8] for i in range(len(idxs)-8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in_dat = c_dat[:-1]\n",
    "c_out_dat = c_dat[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[35, 65, 7, 0, 41, 63, 52, 48],\n",
       "  [65, 7, 0, 41, 63, 52, 48, 58],\n",
       "  [7, 0, 41, 63, 52, 48, 58, 52],\n",
       "  [0, 41, 63, 52, 48, 58, 52, 65],\n",
       "  [41, 63, 52, 48, 58, 52, 65, 5]],\n",
       " [[65, 7, 0, 41, 63, 52, 48, 58],\n",
       "  [7, 0, 41, 63, 52, 48, 58, 52],\n",
       "  [0, 41, 63, 52, 48, 58, 52, 65],\n",
       "  [41, 63, 52, 48, 58, 52, 65, 5],\n",
       "  [63, 52, 48, 58, 52, 65, 5, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if everything looks okay\n",
    "c_in_dat[:5], c_out_dat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1027432, 8), (1027432, 8))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.stack(c_in_dat)\n",
    "ys = np.stack(c_out_dat)\n",
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one line allows me to run my models on a subset of data while I work on this notebook\n",
    "md = ColumnarModelData.from_arrays('.', [-20_000], xs[:200_000, :], ys[:200_000], bs=2**9)\n",
    "\n",
    "# md = ColumnarModelData.from_arrays('.', [-100_000], xs, ys, bs=2**9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = 42 # number of latent factors, embedding length\n",
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h0 = V(torch.zeros(1, bs, n_hidden).cuda())\n",
    "        inp = self.e(torch.stack(cs))\n",
    "        output, hn = self.rnn(inp, h0)\n",
    "        return F.log_softmax(self.l_out(output), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqRNN(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, at the time of writing the published pytorch version is still 0.3. That means that the multidimensional nll_loss has not made it into a release yet.\n",
    "\n",
    "Because we will now be outputting a prediction for 8 chars, instead of one, we need to write a wrapper around the pytorch's nll_loss as it only works with outputs of shape (N, C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(inp, targ):\n",
    "    sl,bs,nh = inp.size()\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1)\n",
    "    return F.nll_loss(inp.view(-1,nh), targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49faf1357b1249658ffc9d32c6f050c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       1.70905  1.93887]                                \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd31b518f464b60930a44dbbde7ff49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       1.58125  1.7642 ]                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(m, md, 1, opt, nll_loss_seq)\n",
    "set_lrs(opt, 1e-3)\n",
    "fit(m, md, 1, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce text we can still use nearly the same get_next method as we were using in the past (we just need to make sure we pass it the last prediction our model outputs vs predictions after seeing each char in the sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char2idx[c] for c in inp]))\n",
    "    p = m(*VV(idxs))[-1].exp()\n",
    "    r = torch.multinomial(p, 1)\n",
    "    return idx2char[r.data[0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this time we want to be feeding it a sequence of length <= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloy. Aftart tod businession, wholingly hather remidam Mip. Our peacemer in their deficit to we must to America know their time that is mrlar, thry hese prograf immiare Areamber abrors alreeps that may g\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello'\n",
    "for i in range(200): text += get_next(text[(-8 if len(text) > 7 else 0):])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, though not great, as expected.\n",
    "\n",
    "One minor trick that we can use to help with training is initializing the hidden state to an identity matrix. (I think this should generally be helpful though mileage might vary - [best used with ReLu](https://arxiv.org/abs/1504.00941) and we are using tanh here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqRNN(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    1     0     0  ...      0     0     0\n",
       "    0     1     0  ...      0     0     0\n",
       "    0     0     1  ...      0     0     0\n",
       "       ...          ⋱          ...       \n",
       "    0     0     0  ...      1     0     0\n",
       "    0     0     0  ...      0     1     0\n",
       "    0     0     0  ...      0     0     1\n",
       "[torch.cuda.FloatTensor of size 256x256 (GPU 0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3a8a8c8212483e9b7019fb65a63804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       1.75226  2.50406]                                \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc97da7b00cc46cebc004dd2db9146ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       1.62457  2.15972]                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(m, md, 1, opt, nll_loss_seq)\n",
    "set_lrs(opt, 1e-3)\n",
    "fit(m, md, 1, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make another addition to our toolbox - making the RNN stateful. If you would like watch Jeremy explain this in detail, this is covered in [lecture #7](https://youtu.be/H3g26EVADgY?t=6m13s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of discarding the hidden state after each batch we will retain it to have a better starting point for the next batch. This will requires our dataset to be constructed in a very specific way. This also means that we have to somehow preventing backpropagating the error through multiple batches (otherwise we will get a NN with thousands of layers and that is not what we want).\n",
    "\n",
    "Instead of writing our own dataset machinery, let's employ torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mCorpus of Presential Speeches\u001b[0m/  \u001b[01;31mcorpus.zip\u001b[0m  \u001b[01;34mmodels\u001b[0m/  \u001b[01;34mtrn\u001b[0m/  \u001b[01;34mval\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "# if you have not used spacy before you might need to download the english model\n",
    "# via executing the below from terminal\n",
    "#   python -m spacy download en\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "PATH='data/'\n",
    "\n",
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "!mkdir -p {TRN}\n",
    "!mkdir -p {VAL}\n",
    "\n",
    "with open(f'{TRN}trn.txt', 'w') as text_file:\n",
    "    text_file.write(speeches[:800_000])\n",
    "    \n",
    "with open(f'{VAL}val.txt', 'w') as text_file:\n",
    "    text_file.write(speeches[800_000:])\n",
    "\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data using the functionality provided by fastai and torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780, 75, 1, 800001)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(lower=False, tokenize=list)\n",
    "bs=128; bptt=8; n_fac=42; n_hidden=256\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, maybe this worked, maybe it didn't :) Let's see what we get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([11, 128]), torch.Size([1408]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(md.trn_dl)\n",
    "batch = next(it)\n",
    "len(batch), batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so this is nice. The number of characters in a batch will slightly vary and also seems our targets are unrolled into a single dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size # we will need it to modify the output of our RNN to work with F.nll_loss\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs) # this is new - we will initialize the hidden state once, at model creation vs\n",
    "                             # reinitializing it at the beginning of ever batch\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs) # in case our last batch contains a lesser number of rows\n",
    "        inp = self.e(cs)\n",
    "        output, hn = self.rnn(inp, self.h)\n",
    "        self.h = repackage_var(hn) # repackaging a var is a fastai function for h to forget history\n",
    "        return F.log_softmax(self.l_out(output), dim=-1).view(-1, self.vocab_size) # this will allow us to use F.nll_loss\n",
    "\n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulRNN(md.nt, n_fac, bs).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf240e592c794d96a54753dbae18f6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       1.68157  1.71193]                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f428f2f00f47699c67ccf03ffb63ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       1.43695  1.44754]                                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)\n",
    "set_lrs(opt, 1e-3)\n",
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model now expexts an input of shape (seq_len, batch, input_size), we need to modify our `get_next` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))[-1].exp()\n",
    "    r = torch.multinomial(p, 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellow, those violence. So faition-was an corports work diden with tax make quillyy passnd issue just years detent that next papreas. Let this drities, even the fad this purstition and scould is un righem \n"
     ]
    }
   ],
   "source": [
    "text = 'Hello'\n",
    "for i in range(200): text += get_next(text[(-8 if len(text) > 7 else 0):])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [lecture video](https://youtu.be/H3g26EVADgY?t=47m19s) Jeremy provides a great explanation of how the GRU and LSTM cells work.\n",
    "\n",
    "There are also great tutorials available. I learned about them from the wiki for lesson #7 on the [fast.ai forums](http://forums.fast.ai/t/wiki-lesson-7/9405).\n",
    "\n",
    "I reference them here for your convenience:\n",
    "\n",
    "* [Chris Olah on LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [WILD ML RNN Tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "\n",
    "I will not go into the details of how the cells work. If you are interested the materials above are the best source to study this further.\n",
    "\n",
    "The good news is that we already know everything we need to start using either the GRU or the LSTM cell. From outside they both are very similar to the nn.RNN layer.\n",
    "\n",
    "There are two things that still remain. Fully training the model using one of the more complex cells and implementing beam search.\n",
    "\n",
    "Let's start with constructing and training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM model\n",
    "\n",
    "First of all, we the data. This time around, let's lower the batch size but increase the bptt significantly.\n",
    "\n",
    "Increasing the bptt will allow our RNN to learn from longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 75, 1, 800001)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(lower=False, tokenize=list)\n",
    "bs=64; bptt=72; n_fac=42;\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 512 # Let's make our hidden state slightly bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Char_LSTM_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.nl = nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs) # in case our last batch contains a lesser number of rows\n",
    "        inp = self.e(cs)\n",
    "        output, hn_and_cell_state = self.rnn(inp, self.h)\n",
    "        self.h = repackage_var(hn_and_cell_state) # repackaging a var works with a tuple as well!\n",
    "        return F.log_softmax(self.l_out(output), dim=-1).view(-1, self.vocab_size) # this will allow us to use F.nll_loss\n",
    "\n",
    "    def init_hidden(self, bs): \n",
    "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, n_hidden))) # we need this now because LSTM expects to be given both\n",
    "                                                   # the initial hidden state but also the cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using the fastai library, we have access to all of its goodies. This means we are just a few keystrokes away from using the wonderful cosine annealing of the learning rate!\n",
    "\n",
    "And we can also save the intermediate model at cycle ends. We could probably further improve perofrmance via ensembling a couple of the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Char_LSTM_RNN(md.nt, n_fac, bs, 3).cuda()\n",
    "lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6265ef984bb84f32835dc32e6bf2084d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       2.68748  2.53559]                                \n",
      "[ 1.       2.03807  1.80079]                                \n",
      "[ 2.       1.70679  1.61632]                                \n",
      "[ 3.       1.58691  1.46066]                                \n",
      "[ 4.       1.44753  1.34418]                                \n",
      "[ 5.       1.35991  1.28386]                                \n",
      "[ 6.       1.3099   1.26304]                                \n",
      "[ 7.       1.41269  1.32544]                                \n",
      "[ 8.       1.37102  1.29049]                                \n",
      "[ 9.       1.32688  1.25503]                                \n",
      "[ 10.        1.28493   1.22384]                             \n",
      "[ 11.        1.24576   1.19582]                             \n",
      "[ 12.        1.20741   1.1722 ]                             \n",
      "[ 13.        1.17764   1.1587 ]                             \n",
      "[ 14.        1.16298   1.15617]                             \n",
      "[ 15.        1.31912   1.25402]                             \n",
      "[ 16.        1.30095   1.23091]                             \n",
      "[ 17.        1.28025   1.21963]                             \n",
      "[ 18.        1.26712   1.21154]                             \n",
      "[ 19.        1.24785   1.198  ]                             \n",
      "[ 20.        1.23267   1.19142]                             \n",
      "[ 21.        1.21124   1.17077]                             \n",
      "[ 22.        1.19006   1.16204]                             \n",
      "[ 23.        1.16817   1.1488 ]                             \n",
      "[ 24.        1.15097   1.13625]                             \n",
      "[ 25.        1.12642   1.12391]                             \n",
      "[ 26.        1.10878   1.11726]                             \n",
      "[ 27.        1.08956   1.10924]                             \n",
      "[ 28.        1.07533   1.10588]                             \n",
      "[ 29.        1.06362   1.10317]                             \n",
      "[ 30.        1.05787   1.10126]                             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**5-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now trained our model. We could explore the architecture a bit more (add hiden layers, change embedding size, stack LSTMs) but the defaults taken from the [fastai lesson 6 notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson6-rnn.ipynb) work quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to learn more about ways to do search (depth / breadth first search, hill climbing, beam search), this is a great [lecture to watch](https://www.youtube.com/watch?v=j1H3jAAGlEA)\n",
    "\n",
    "This [2 minute video](https://www.youtube.com/watch?v=UXW6Cs82UKo) from Udacity talks about beam search only.\n",
    "\n",
    "I will implement the beam search using basic data structures only, such as lists and tuples.\n",
    "\n",
    "But first, let's modify the `get_next` function so that it returns both the predicted character and its probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    '''Return predicted char and the probability of prediction'''\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))[-1].exp()\n",
    "    r = torch.multinomial(p, 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]], p[r].data.cpu()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with a breadth first search. Going from it to beam search is trivial.\n",
    "\n",
    "Breadth first search would allow us ot explore the entire solution space. But at a cost. If we were to generate a string of length 10 trying out each possible combination of characters, that would give us 75^10 = 5631351470947265625 possible solutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breadth_first_search(texts, steps):\n",
    "    '''\n",
    "    Args:\n",
    "        texts: a list of tuples of the form (<text>, <probability>)\n",
    "        steps: steps to go\n",
    "    '''\n",
    "    if steps == 0: return texts\n",
    "    \n",
    "    new_texts = []\n",
    "    \n",
    "    for text, prob_text in texts:\n",
    "        dist = get_distribution(text)\n",
    "        for char, prob_char in dist:\n",
    "            new_texts.append((text + char, prob_text + prob_char))\n",
    "    \n",
    "    new_texts = sorted(new_texts, key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    return breadth_first_search(new_texts, steps - 1)\n",
    "\n",
    "def get_distribution(inp):\n",
    "    idxs = np.array([TEXT.vocab.stoi[c] for c in inp])\n",
    "    idxs = idxs.reshape(-1, 1) # RNNs in PyTorch expect input of dim [seq_len x batch_size x embedding_size]\n",
    "    p = m(VV(idxs))[-1]\n",
    "    chars_with_prob = zip(TEXT.vocab.itos, p.data.cpu().numpy())\n",
    "    return sorted(chars_with_prob, key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 421875 3 char combinations\n",
      "CPU times: user 6.09 s, sys: 2.2 s, total: 8.29 s\n",
      "Wall time: 8.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = breadth_first_search([('I am', 0)], 3)\n",
    "print(f'Found {len(result)} 3 char combinations')\n",
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the best of my understanding, the implementation of beam search below is correct. It unfortunately was not giving great results - the outputs were too conservative. The serach was producing text that had high probability according to the RNN but with a lot repetitions.\n",
    "\n",
    "I came up with a `stochastic beam search` where the results are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(texts, steps, beam_size):\n",
    "    '''\n",
    "    Args:\n",
    "        texts: a list of tuples of the form (<text>, <probability>)\n",
    "        steps: steps to go\n",
    "        beam_size: count of candidate branches to keep\n",
    "    '''\n",
    "    if steps == 0: return texts\n",
    "    \n",
    "    new_texts = []\n",
    "    \n",
    "    for text, prob_text in texts:\n",
    "        dist = get_distribution(text)\n",
    "        for char, prob_char in dist:\n",
    "            new_texts.append((text + char, prob_text + prob_char))\n",
    "    \n",
    "    new_texts = sorted(new_texts, key=lambda tup: tup[1], reverse=True)[:beam_size]\n",
    "    \n",
    "    return beam_search(new_texts, steps - 1, beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_beam_search(texts, steps, beam_size, candidates_to_evaluate):\n",
    "    '''\n",
    "    Args:\n",
    "        texts: a list of tuples of the form (<text>, <log_probability>)\n",
    "        steps: steps to go\n",
    "        beam_size: count of candidate branches to keep\n",
    "        candidates_to_evaluate: how many times should we expand a given branch at each step\n",
    "    '''\n",
    "    if steps == 0: return texts[0][0]\n",
    "    new_texts = []\n",
    "    \n",
    "    for text, prob_text in texts:\n",
    "        for _ in range(candidates_to_evaluate):\n",
    "            c, p = get_next(text)\n",
    "            new_texts.append((text + c, prob_text + np.log(p)))\n",
    "        \n",
    "    new_texts = sorted(new_texts, key=lambda tup: tup[1], reverse=True)[:beam_size]\n",
    "    return stochastic_beam_search(new_texts, steps - 1, beam_size, candidates_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.29 s, sys: 1.31 s, total: 10.6 s\n",
      "Wall time: 10.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The meaning of life is that we are a better strength of college than the law in our own, and the world of the depends of a decade of interests who are a single lives of a training and the future that despite its ideas. And they don't take the country in the world's faith to our economy. We will not threaten a solve the facts of the same possible challenges that the community is a safe haven in the more than the same res\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "stochastic_beam_search([('The meaning of life is ', 0)], 400, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we repeat the above but with beam of size 1, which in our case equates to sampling directly from the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.5 s, sys: 804 ms, total: 5.3 s\n",
      "Wall time: 5.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The meaning of life is both pointing, when we worked without extremists for me set answer and everyone faced to do onting idea up nuclear very doubts how because we will change the weeks. We is containing together. We need so much family by the history. So America needs to be areay with issues, no coal that makes businesss. We'll defend our applying prayers and good with worker or a char-off religion to ockane, what wou\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "stochastic_beam_search([('The meaning of life is ', 0)], 400, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the stochastic beam search (a name I made up) would be the winner but hard to say. One thing that might be interesting to explore would be prunning not after n + 1 chars, but after n + i for some i > 1. This might allow for greater variety of branches that make it into the beam. We might also be able to prune branches that turn out bad only further downstream.\n",
    "\n",
    "As generating sequences is relatively fast, one could also consider generating some number of sequences sampling directly from the softmax and prunning afterwards. For example, we could generate 20 strings and pick the one that the RNN believest to have greates probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here\n",
    "\n",
    "I would definitely recommend reading an article by Andrej Karpathy on [the unreasonable effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) that has now become a classic.\n",
    "\n",
    "With basic understanding of RNNs and the inner workings of the GRU and the LSTM cells, the world is an oyster. Really.\n",
    "\n",
    "There are so many architectures to explore. Bi-directional RNNs, seq2seq, attention models. You might jump into exploring the field on your own or if you prefer having an experienced guide, I would recommend checking out the [Cutting Edge Deep Learning for Coders](http://course.fast.ai/part2.html) by [fastai](http://www.fast.ai/).\n",
    "\n",
    "And this is it! I do not envision there being a part 3, but the list of things I would like to work on and potentially write about is at this point endless :) See you on [twitter](https://twitter.com/radekosmulski) or on the [forums](http://forums.fast.ai/)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
